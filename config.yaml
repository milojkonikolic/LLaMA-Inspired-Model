train_data: /hdd/Datasets/WikiText-2/data/train.txt
eval_data: /hdd/Datasets/WikiText-2/data/validation.txt
gpu_id: 0
batch_size: 2
epochs: 2
learning_rate: 0.0001
log_interval: 100
output_dir: /hdd/train_logs/LLaMA-Inspired-Model/run4
vocab_size: 50257
embedding_dim: 256
context_length: 256
num_heads: 8
num_decoders: 8
